<!DOCTYPE html>
<html lang="es">
<head>
    <link href="css/css/bootstrap.min.css" rel="stylesheet">
    <script src="css/js/bootstrap.min.js"></script>


    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.9/css/all.css"
        integrity="sha384-5SOiIsAziJl6AWe0HWRKTXlfcSHKmYV4RBF18PPJ173Kzn7jzMyFuTtk8JA7QQG1" crossorigin="anonymous">

    <style>
        header{
            position: fixed;
            width: 100%;
            z-index: 1000;
        }
    </style>
</head>

<body>
    <header>
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
            <div class="container-fluid">
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                    aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                        <li class="nav-item">
                            <a class="nav-link" aria-current="page" href="#">Inicio</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#proyectos">Proyectos</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#publicaciones">Publicaciones</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#codigos">Códigos</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link disabled" href="#">Cursos</a>
                        </li>
                    </ul>

                </div>
            </div>
        </nav>
    </header>
    <br>
    <div class="jumbotron feature">
        <div class="container">
            <div class="col-sm-12 col-md-12">
                <!--<div class="col-sm-12 col-md-3">
                    <span class="helper"></span>
                    <img class="img-responsive img-rounded" src="images/profile.jpg"  alt="">
                </div>-->
                <div class="row"> <br> <br></div>
                <div class="row">
                    <h2>Carlos Ismael Orozco</h2>
                    <p>
                        <a href="https://www.linkedin.com/in/carlos-ismael-orozco-8bb4b3222/" target="_blank"> <i
                                class="fab fa-linkedin"></i> </a>
                        <a href="mailto:ciorozco.unsa@gmail.com" target="_top"> <i class="fas fa-envelope"></i></a>
                        #computerVision #NLP #deepLearning #machineLearning #webDevelopment
                    </p>
                </div>

                <div class="row">
                    <p align="justify">
                        <b>Resumen</b> Lic. en Análisis de Sistemas. Universidad Nacional de Salta (2013). Investigador
                        Categoría V (Ingeniería)
                        de acuerdo a la Secretaría de Políticas Universitarias SPU. Actualmente Doctorando en Ciencias
                        de la Computación - Universidad de
                        Buenos Aires. Argentina. Grupo de Investigación Procesamiento de Imágenes y Visión por
                        Computadora. Entre mis áreas de
                        interés se destacan el procesamiento de imágenes, redes neuronales profundas y desarrollo web. Me desempeño como
                        Jefe de Trabajos Prácticos en el Departamento de Informática. Universidad Nacional de Salta.
                    </p>

                    <div>
                        <h5> Lenguajes de Programación</h5>
                        <div style="width: 80%; margin:auto;">
                            <div class="progress" style="height: 22px;">
                                <div class="progress-bar" role="progressbar" style="width: 84%;" aria-valuenow="25"
                                    aria-valuemin="0" aria-valuemax="100"> Python</div>
                            </div>
                            <br>
                            <div class="progress" style="height: 22px;">
                                <div class="progress-bar" role="progressbar" style="width: 73%;" aria-valuenow="25"
                                    aria-valuemin="0" aria-valuemax="100"> PHP - Laravel</div>
                            </div>
                            <br>
                            <div class="progress" style="height: 22px;">
                                <div class="progress-bar" role="progressbar" style="width: 63%;" aria-valuenow="25"
                                    aria-valuemin="0" aria-valuemax="100"> React - Angular - NodeJS</div>
                            </div>
                            <br>
                            <div class="progress" style="height: 22px;">
                                <div class="progress-bar" role="progressbar" style="width: 65%;" aria-valuenow="25"
                                    aria-valuemin="0" aria-valuemax="100"> HTML - CSS3 - Bootstrap - Javascript - JQuery</div>
                            </div>
                            <br>
                            <div class="progress" style="height: 22px;">
                                <div class="progress-bar" role="progressbar" style="width: 70%;" aria-valuenow="25"
                                    aria-valuemin="0" aria-valuemax="100"> MySQL - PostgreSQL - MongoDB</div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>
    <br>   
    <div class="container">

        <!-- Proyectos -->
        <div class="row" id="proyectos"> <br><br><br> </div>
        <div class="row">
            <div class="col-lg-12">
                <h3 class="page-header"><i class="far fa-bookmark"></i> Proyectos </h3>
            </div>
        </div>
        <br>
        <div class="row">
            <article class="col-md-2 article-intro">
                <span class="helper"></span>
                <img class="img-responsive img-rounded" src="images/se-911.jpg" width="140px" height="140px">
            </article>

            <artinfo class="col-sm-12 col-md-10">
                <h5>[2021] Convenio de Colaboración DI-UNSa y Sistema de Emergencias 911.
                <a href="pdfs/R-DR-2021-0626.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                </a> </h5>
                <p align="justify">
                    Los sistemas de reconocimiento de emociones del habla (SER) tienen como objetivo identificar el estado emocional de una persona analizando únicamente su voz, es decir, el sistema deberá seleccionar una clase (alegre, enojado, triste, miedo, sorpresa, etc.) aquella que sea más probable para el audio de entrada. SER es un tema de interés en el área de procesamiento digital de audio debido a las potenciales aplicaciones que se pueden desarrollar, por ejemplo: sistemas que interactúan con el humano en base a las emociones percibidas, asistentes en terapias psicológicas, detección de mentiras en interrogatorios, entre otros. En este convenio de Cooperación para el desarrollo de una Aplicación Web para el Reconocimiento de Emociones
                    en el Habla a través de Audios.
                </p>
            </artinfo>
        </div>
        <br>
        <div class="row">
            <article class="col-md-2 article-intro">
                <span class="helper"></span>
                <img class="img-responsive img-rounded" src="images/compra_en_tu_barrio.jpeg" width="120px"
                    height="120px">
            </article>

            <artinfo class="col-sm-12 col-md-10">
                <h5>
                    [2020] Convenio de Colaboración DI-UNSa y Municipalidad de la Ciudad de Salta.
                    <a href="pdfs/R-DR-2020-1070.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                </h5>
                <p align="justify">
                    Convenio de Cooperación a través del Ente de Desarrollo Económico, Formación y Capacitación para la
                    Promoción de Empleo, para la mejora tecnológica de la <a target="_blank"
                        href="https://compraentubarrio.gob.ar">Plataforma Comprá en tu Barrio </a>.
                </p>
            </artinfo>
        </div>
        <br>
        <div class="row">
            <article class="col-md-2 article-intro">
                <span class="helper"></span>
                <img class="img-responsive img-rounded" src="images/ucm.png" width="100px" height="100px">
            </article>

            <artinfo class="col-sm-12 col-md-10">
                <h5>
                    [2016] Pasantía de Investigación. LITRP, Universidad Católica del Maule. Talca - Chile.
                    <a href="https://arxiv.org/abs/1912.05019" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                </h5>
                <p align="justify">
                    El Laboratorio de Investigación Tecnológica en Reconocimiento de Patrones (LITRP), es uno de los
                    laboratorios de investigación del Edificio Parque Científico-Tecnológico de la Universidad Católica
                    del Maule. En la pasantía implemente un algoritmo de aprendizaje automático para la clasificación de
                    imágenes correspondiente a un conjunto de semillas de uvas en inmaduras, maduras y sobremaduras,
                    usando la información a escala de grises. La evidencia industrial indica que la cosecha de las uvas
                    en su etapa maduras produce un vino de mejor calidad. <br>
                    <b> Tema: Wine Grape Maturity Estimation using a Seed Grayscale Chart. </b><br>
                    Director: Dr. Marco Mora
                </p>
            </artinfo>
        </div>

        <!-- Publicaciones -->
        <div class="row" id="publicaciones"> <br><br><br> </div>
        <div class="row">
            <div class="col-lg-12">
                <h3 class="page-header"><i class="far fa-bookmark"></i> Publicaciones </h3>
            </div>
        </div>
        <br>
        <div class="row">
            <artinfo class="col-sm-12 col-md-12">
                <p>
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <b>Spatial Attention Adapted to a LSTM Architecture with Frame Selection for Human Action
                        Recognition in Videos.</b>
                    Carlos Ismael Orozco. María Elena Buemi. Julio Jacobo Berlles.
                    LatinX - 38th International Conference on Machine Learning ICML 2021.
                    <a href="pdfs/2021_icml.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                    <br><br><i >
                        Resumen: El reconocimiento de acciones en videos es actualmente un tema de interés en el área de la visión artificial, debido a sus potenciales aplicaciones como: indexación multimedia, vigilancia en espacios públicos, entre otras. En este trabajo proponemos un mecanismo de atención adaptado a una arquitectura base CNN-LSTM. Para llevar a cabo las fases de entrenamiento y prueba, utilizamos los conjuntos de datos HMDB-51 y UCF-101. Evaluamos el desempeño de nuestro sistema usando la precisión como métrica de evaluación, obteniendo 57.3% y 90.4% para HMDB-51 y UCF-101 respectivamente.
                    </i>
                    
                </p>
            </artinfo>
        </div>

        <div class="row">
            <artinfo class="col-sm-12 col-md-12">
                <p>
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <b>CNN-LSTM with soft attention mechanism for human action recognition in videos</b>
                    Carlos Ismael Orozco. María Elena Buemi. Julio Jacobo Berlles.
                    Elektron: Ciencia y tecnología en la electrónica de hoy. Vol. 5, No. 1, pp. 37-44.
                    ISSN 2525-0159. DOI: https://doi.org/10.37537/rev.elektron.5.1.130.2021. Año 2021.
                    <a href="pdfs/2021_elektron.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                    <br><br><i>
                        Resumen: El reconocimiento de acciones en videos es actualmente un tema de interés en el área de la visión por computador, debido a potenciales aplicaciones como: indexación multimedia, vigilancia en espacios públicos, entre otras. Los mecanismos de atención se han convertido en un concepto muy importante dentro del enfoque de aprendizaje profundo, su operación intenta imitar la capacidad visual de las personas que les permite enfocar su atención en partes relevantes de una escena para extraer información importante. En este artículo proponemos un mecanismo de atención suave adaptado para degradar la arquitectura CNN--LSTM. Primero, una red neuronal convolucional VGG16 extrae las características del video de entrada. Para llevar a cabo las fases de entrenamiento y prueba, usamos los conjuntos de datos HMDB-51 y UCF-101. Evaluamos el desempeño de nuestro sistema usando la precisión como métrica de evaluación, obteniendo 40.7% (enfoque base), 51.2% (con atención) para HMDB-51 y 75.8% (enfoque base), 87.2% (con atención) para UCF-101.
                    </i>
                </p>
            </artinfo>
        </div>

        <div class="row">
            <artinfo class="col-sm-12 col-md-12">
                <p align="justify">
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <b> COVID-XR: A Web Management Platform for Coronavirus Detection on X-ray Chest Images.</b> Carlos
                    Ismael Orozco. Eduardo Xamena. Cristian Martínez. Diego Rodriguez.
                    Revista IEEE Latin America Transactions, 100(1e). Factor de impacto 0.8. Año 2020. 
                    <a href="pdfs/2020_ieeelat.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                    <br><br><i>
                        Resumen: COVID-19 es una enfermedad infecciosa causada por el virus SARS-CoV-2. Sus síntomas son similares a los de la gripe común, incluyendo fiebre, tos, disnea, mialgia y fatiga. Debido a su rápida expansión a nivel mundial, la Organización Mundial de la Salud (OMS) la declaró pandemia. La prueba molecular comúnmente utilizada en todo el mundo para la detección directa del virus es la prueba RT-PCR, pero su procesamiento lleva tiempo y los materiales utilizados son escasos. En este trabajo proponemos: (a) El diseño e implementación de una arquitectura de red neuronal profunda para la detección de pacientes con COVID-19 utilizando como entrada imágenes de rayos X de tórax; la arquitectura se compone de una fase de extracción de características, es decir, un modelo VGG16 pre-entrenado extrae las características de la imagen; luego, en la segunda fase, una red neuronal multicapa se clasifica en una de dos clases particulares (1: COVID, 0: NO COVID). (b) La implementación de una plataforma Web que permita a las personas interesadas utilizar nuestra arquitectura de manera clara, sencilla y transparente. El algoritmo de aprendizaje profundo se implementó en Python con librerías específicas para el diseño de redes neuronales, mientras que la plataforma Web se implementó en PHP usando el framework Laravel y base de datos MySQL. Evaluamos el desempeño de nuestra propuesta utilizando las métricas de evaluación de sensibilidad, especificidad y área bajo la curva (AUC), obteniendo buenos resultados en tiempos computacionales muy cortos.
                    </i>
                </p>
            </artinfo>
        </div>

        <div class="row">
            <artinfo class="col-sm-12 col-md-12">
                <p align="justify">
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <b> End-to-end platform evaluation for Spanish Handwritten Text Recognition. </b> Eduardo Xamena.
                    Carlos Ismael Orozco. Emanuel Barboza.
                    Ciencia y Tecnología, 81-95. ISSN 2344-9217.
                    <a href="pdfs/2020_ieeelat.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                    <br><br><i>
                        Resumen: La tarea del reconocimiento automatizado de textos manuscritos
                        requiere de diversas fases y tecnologías tanto ópticas como del lenguaje. En
                        este artículo se describe un enfoque para la realización de esta tarea de
                        forma completa, mediante el empleo de aprendizaje automatizado a lo largo
                        de todas las fases del proceso. Además de explicar la metodología
                        empleada, se describe el proceso de construcción y evaluación de un modelo
                        de reconocimiento de manuscritos para el lenguaje español. La contribución
                        original de este artículo está dada por el entrenamiento y evaluación de
                        modelos de Offline HTR para manuscritos en español, así como la evaluación de una plataforma para la realización de esta tarea de forma
                        completa. Además, se detallan los trabajos que se están llevando a cabo para
                        lograr mejoras en los modelos obtenidos, y desarrollar nuevos modelos para
                        distintos corpus de lectura compleja.
                    </i>
                </p>
            </artinfo>
        </div>

        <div class="row">
            <artinfo class="col-sm-12 col-md-12">
                <p align="justify">
                    <i class="far fa-file-alt pub-icon" aria-hidden="true"></i>
                    <b> Real-Time Gender Recognition From Face Images Using Deep Convolutional Neural Network. </b> Carlos Ismael Orozco. Florencia Iglesias. María Elena Buemi. Julio Jacobo Berlles.
                    Ciencia y Tecnología, 81-95. ISSN 2344-9217.
                    <a href="pdfs/2017_ICPRS.pdf" data-toggle="tooltip" title="Download" target="_blank">
                        <i class="fas fa-download"></i>
                    </a>
                    <a href="https://www.youtube.com/watch?v=Tfi-p2K9NBg" data-toggle="tooltip" title="Video"
                        target="_blank">
                        <i class="fab fa-youtube"></i>
                    </a>
                    <br><br><i>
                        Resumen: El reconocimiento de género es un tema de interés en visión por computadora debido a sus aplicaciones como vigilancia en lugares públicos, publicidad dirigida, entre otras. Los buenos resultados obtenidos utilizando redes neuronales convolucionales profundas en tareas de visión las convierten en una herramienta atractiva para mejorar las capacidades de los sistemas de reconocimiento de género. En este trabajo, proponemos una arquitectura de red convolucional profunda para clasificar como persona masculina o femenina las regiones candidatas detectadas previamente utilizando características de Haar integradas en un AdaBoost. El conjunto de datos utilizado para el entrenamiento y las pruebas proviene del conjunto de datos de Labeled Faces in the Wildand Gallagher. Hemos evaluado los resultados de la clasificación sobre la arquitectura propuesta y hemos obtenido un promedio de ∼ 95,42% y ∼ 91,48% de precisión para el conjunto de entrenamiento y para el conjunto de prueba, respectivamente, que son competitivos con los mencionados en la bibliografía. También hemos llevado a cabo una evaluación en tiempo real del sistema mediante una cámara web.
                    </i>
                </p>
            </artinfo>
        </div>
    
     <!-- Códigos -->
     <div class="row" id="codigos"> <br><br><br> </div>
     <div class="row">
         <div class="col-lg-12">
             <h3 class="page-header"><i class="far fa-bookmark"></i> Códigos </h3>
         </div>
     </div>
     <br>
     <div class="row">
         <artinfo class="col-sm-12 col-md-12">
             <p>
                 <i class="fas fa-code" aria-hidden="true"></i>
                 <b>Desarrollo Web </b>(NodeJS + Angular + MongoDB): Portafolio Personal 
                 <a href="https://github.com/ciorozco/portafolio" data-toggle="tooltip" title="Code"
                     target="_blank">
                     <i class="fas fa-link"></i>
                 </a>
             </p>
         </artinfo>
     </div>
     <div class="row">
        <artinfo class="col-sm-12 col-md-12">
            <p>
                <i class="fas fa-code" aria-hidden="true"></i>
                <b>Procesamiento de imágenes </b> (Python): Detección de Mal de Parkinson 
                <a href="https://github.com/ciorozco/parkinson" data-toggle="tooltip" title="Code"
                    target="_blank">
                    <i class="fas fa-link"></i>
                </a>
            </p>
        </artinfo>
    </div>
    <div class="row">
        <artinfo class="col-sm-12 col-md-12">
            <p>
                <i class="fas fa-code" aria-hidden="true"></i>
                <b>Procesamiento de texto </b> (Python): Detección de desastres naturales en Twitter 
                <a href="https://github.com/ciorozco/desastres-naturales" data-toggle="tooltip" title="Code"
                    target="_blank">
                    <i class="fas fa-link"></i>
                </a>
            </p>
        </artinfo>
    </div>
    </div>
</body>

</html>